{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DL autoencoder.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"8_XWnMCBZ3Bl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"f973c463-06a2-427e-8d33-8766538ce6a3","executionInfo":{"status":"ok","timestamp":1542491402765,"user_tz":300,"elapsed":357,"user":{"displayName":"DANIEL MARCELO CHAPI ALEJO","photoUrl":"https://lh4.googleusercontent.com/-0qTgCC4gOYw/AAAAAAAAAAI/AAAAAAAAAAo/xbudxPyl1Uk/s64/photo.jpg","userId":"06845776485992914193"}}},"cell_type":"code","source":["%load_ext autoreload\n","%autoreload 2"],"execution_count":18,"outputs":[{"output_type":"stream","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"],"name":"stdout"}]},{"metadata":{"id":"jINkXrBEZ7KX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"cee10a15-bf03-41b3-cf70-5df674410031","executionInfo":{"status":"ok","timestamp":1542491404266,"user_tz":300,"elapsed":348,"user":{"displayName":"DANIEL MARCELO CHAPI ALEJO","photoUrl":"https://lh4.googleusercontent.com/-0qTgCC4gOYw/AAAAAAAAAAI/AAAAAAAAAAo/xbudxPyl1Uk/s64/photo.jpg","userId":"06845776485992914193"}}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"metadata":{"id":"vai4Ze85aB0U","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"outputId":"9fa476e8-ff76-431a-dde0-33bd16ec59ab","executionInfo":{"status":"ok","timestamp":1542491409987,"user_tz":300,"elapsed":1550,"user":{"displayName":"DANIEL MARCELO CHAPI ALEJO","photoUrl":"https://lh4.googleusercontent.com/-0qTgCC4gOYw/AAAAAAAAAAI/AAAAAAAAAAo/xbudxPyl1Uk/s64/photo.jpg","userId":"06845776485992914193"}}},"cell_type":"code","source":["!ls drive/'My Drive'/ML/myanimelist/"],"execution_count":20,"outputs":[{"output_type":"stream","text":["anime_cleaned.csv\t animelists_reduced.csv  users_cleaned.gsheet\n","anime_filtered.csv\t animes_reduced.csv\t users_filtered.csv\n","AnimeList.csv\t\t UserAnimeList.csv\t users_reduced.csv\n","animelists_cleaned.csv\t UserList.csv\n","animelists_filtered.csv  users_cleaned.csv\n"],"name":"stdout"}]},{"metadata":{"id":"hc0EOYL3aToO","colab_type":"text"},"cell_type":"markdown","source":["## Reducimos el tamaño del dataset antes de importarlo"]},{"metadata":{"id":"rTxFpSD2aIhg","colab_type":"code","colab":{}},"cell_type":"code","source":["import csv\n","with open('/content/drive/My Drive/ML/myanimelist/users_cleaned.csv', 'r') as inp, open('/content/drive/My Drive/ML/myanimelist/users_reduced.csv', 'wt') as out:\n","    writer = csv.writer(out)\n","    i=0\n","    for row in csv.reader(inp):\n","      i+=1\n","      writer.writerow(row)\n","      if(i==200):\n","        break"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WP7JFBFTaR2d","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"40cec63e-7276-49c0-98d5-fd6d46582b89","executionInfo":{"status":"ok","timestamp":1542487585884,"user_tz":300,"elapsed":160439,"user":{"displayName":"DANIEL MARCELO CHAPI ALEJO","photoUrl":"https://lh4.googleusercontent.com/-0qTgCC4gOYw/AAAAAAAAAAI/AAAAAAAAAAo/xbudxPyl1Uk/s64/photo.jpg","userId":"06845776485992914193"}}},"cell_type":"code","source":["import csv\n","\n","usersListed=0\n","with open('/content/drive/My Drive/ML/myanimelist/animelists_cleaned.csv', 'r') as animeLists, open('/content/drive/My Drive/ML/myanimelist/users_reduced.csv', 'r') as usersRed, open('/content/drive/My Drive/ML/myanimelist/animelists_reduced.csv', 'wt') as out:\n","  writer = csv.writer(out)\n","  readerAL = csv.reader(animeLists)\n","  iant=0\n","  userAnt=\"\"\n","  found=0\n","  for rowAL in readerAL:\n","    if(userAnt==rowAL[0] and found==0): continue\n","    found=0\n","    usersRed.seek(0)\n","    users = csv.reader(usersRed)\n","    i=0\n","    userAnt= rowAL[0]\n","    for rowU in users:\n","      i+=1\n","      if(rowAL[0]==rowU[0]):\n","        writer.writerow(rowAL)\n","        \n","        if(i-1!=iant):\n","          #print(i,rowAL[0])\n","          usersListed+=1\n","        iant=i-1\n","        found=1\n","        break\n","print(usersListed)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["199\n"],"name":"stdout"}]},{"metadata":{"id":"SgY39-sIakFc","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","usersListed=0\n","with open('/content/drive/My Drive/ML/myanimelist/animelists_reduced.csv', 'r') as animeLists, open('/content/drive/My Drive/ML/myanimelist/anime_cleaned.csv', 'r') as animes, open('/content/drive/My Drive/ML/myanimelist/animes_reduced.csv', 'wt') as out:\n","  writer = csv.writer(out)\n","  readerA = csv.reader(animes)\n","  i=0\n","  for rowA in readerA:\n","    animeLists.seek(0)\n","    animeL = csv.reader(animeLists)\n","    for rowAL in animeL:\n","      if(rowA[0]==rowAL[1]):\n","        writer.writerow(rowA)\n","        i+=1\n","        #print(i,rowA[1])\n","        break"],"execution_count":0,"outputs":[]},{"metadata":{"id":"paJvwWLqbHfR","colab_type":"text"},"cell_type":"markdown","source":["# **Importamos el dataset**"]},{"metadata":{"id":"6ZgRio7wbFB_","colab_type":"code","colab":{}},"cell_type":"code","source":["import pandas as pd\n","animeUserList = pd.read_csv(\"/content/drive/My Drive/ML/myanimelist/animelists_reduced.csv\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"51hy0at0bLnU","colab_type":"code","colab":{}},"cell_type":"code","source":["users= pd.read_csv(\"/content/drive/My Drive/ML/myanimelist/users_reduced.csv\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"onEj0CgnbND1","colab_type":"code","colab":{}},"cell_type":"code","source":["animes= pd.read_csv(\"/content/drive/My Drive/ML/myanimelist/animes_reduced.csv\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2Vx8Va12bR13","colab_type":"text"},"cell_type":"markdown","source":["# Analizamos el dataset"]},{"metadata":{"id":"0ePGMRp7bQAU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"d7d5ce81-43f8-4fe3-9560-e8c2239df937","executionInfo":{"status":"ok","timestamp":1542487836353,"user_tz":300,"elapsed":422,"user":{"displayName":"DANIEL MARCELO CHAPI ALEJO","photoUrl":"https://lh4.googleusercontent.com/-0qTgCC4gOYw/AAAAAAAAAAI/AAAAAAAAAAo/xbudxPyl1Uk/s64/photo.jpg","userId":"06845776485992914193"}}},"cell_type":"code","source":["print(\"tamaño de usuarios: \", users.shape)\n","print(\"tamaño de animes: \", animes.shape)\n","print(\"tamaño de usuariosXanimes: \", animeUserList.shape)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["tamaño de usuarios:  (199, 17)\n","tamaño de animes:  (4948, 33)\n","tamaño de usuariosXanimes:  (60834, 11)\n"],"name":"stdout"}]},{"metadata":{"id":"XuaycObobXLy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":165},"outputId":"74fcfe58-ae11-49c2-8021-bb01087273fc","executionInfo":{"status":"ok","timestamp":1542487837826,"user_tz":300,"elapsed":409,"user":{"displayName":"DANIEL MARCELO CHAPI ALEJO","photoUrl":"https://lh4.googleusercontent.com/-0qTgCC4gOYw/AAAAAAAAAAI/AAAAAAAAAAo/xbudxPyl1Uk/s64/photo.jpg","userId":"06845776485992914193"}}},"cell_type":"code","source":["users.head(2)"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>username</th>\n","      <th>user_id</th>\n","      <th>user_watching</th>\n","      <th>user_completed</th>\n","      <th>user_onhold</th>\n","      <th>user_dropped</th>\n","      <th>user_plantowatch</th>\n","      <th>user_days_spent_watching</th>\n","      <th>gender</th>\n","      <th>location</th>\n","      <th>birth_date</th>\n","      <th>access_rank</th>\n","      <th>join_date</th>\n","      <th>last_online</th>\n","      <th>stats_mean_score</th>\n","      <th>stats_rewatched</th>\n","      <th>stats_episodes</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>karthiga</td>\n","      <td>2255153</td>\n","      <td>3</td>\n","      <td>49</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>55.091667</td>\n","      <td>Female</td>\n","      <td>Chennai, India</td>\n","      <td>1990-04-29 00:00:00</td>\n","      <td>NaN</td>\n","      <td>2013-03-03 00:00:00</td>\n","      <td>2014-02-04 01:32:00</td>\n","      <td>7.43</td>\n","      <td>0.0</td>\n","      <td>3391</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Damonashu</td>\n","      <td>37326</td>\n","      <td>45</td>\n","      <td>195</td>\n","      <td>27</td>\n","      <td>25</td>\n","      <td>59</td>\n","      <td>82.574306</td>\n","      <td>Male</td>\n","      <td>Detroit,Michigan</td>\n","      <td>1991-08-01 00:00:00</td>\n","      <td>NaN</td>\n","      <td>2008-02-13 00:00:00</td>\n","      <td>2017-07-10 06:52:54</td>\n","      <td>6.15</td>\n","      <td>6.0</td>\n","      <td>4903</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    username  user_id  user_watching  user_completed  user_onhold  \\\n","0   karthiga  2255153              3              49            1   \n","1  Damonashu    37326             45             195           27   \n","\n","   user_dropped  user_plantowatch  user_days_spent_watching  gender  \\\n","0             0                 0                 55.091667  Female   \n","1            25                59                 82.574306    Male   \n","\n","           location           birth_date  access_rank            join_date  \\\n","0   Chennai, India   1990-04-29 00:00:00          NaN  2013-03-03 00:00:00   \n","1  Detroit,Michigan  1991-08-01 00:00:00          NaN  2008-02-13 00:00:00   \n","\n","           last_online  stats_mean_score  stats_rewatched  stats_episodes  \n","0  2014-02-04 01:32:00              7.43              0.0            3391  \n","1  2017-07-10 06:52:54              6.15              6.0            4903  "]},"metadata":{"tags":[]},"execution_count":12}]},{"metadata":{"id":"dxvZIaPebZb3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":734},"outputId":"4c35e1a6-e52c-43a9-9b07-2531770c3200","executionInfo":{"status":"ok","timestamp":1542487898103,"user_tz":300,"elapsed":342,"user":{"displayName":"DANIEL MARCELO CHAPI ALEJO","photoUrl":"https://lh4.googleusercontent.com/-0qTgCC4gOYw/AAAAAAAAAAI/AAAAAAAAAAo/xbudxPyl1Uk/s64/photo.jpg","userId":"06845776485992914193"}}},"cell_type":"code","source":["pd.options.display.max_columns = None\n","display(animes.head())"],"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>anime_id</th>\n","      <th>title</th>\n","      <th>title_english</th>\n","      <th>title_japanese</th>\n","      <th>title_synonyms</th>\n","      <th>image_url</th>\n","      <th>type</th>\n","      <th>source</th>\n","      <th>episodes</th>\n","      <th>status</th>\n","      <th>airing</th>\n","      <th>aired_string</th>\n","      <th>aired</th>\n","      <th>duration</th>\n","      <th>rating</th>\n","      <th>score</th>\n","      <th>scored_by</th>\n","      <th>rank</th>\n","      <th>popularity</th>\n","      <th>members</th>\n","      <th>favorites</th>\n","      <th>background</th>\n","      <th>premiered</th>\n","      <th>broadcast</th>\n","      <th>related</th>\n","      <th>producer</th>\n","      <th>licensor</th>\n","      <th>studio</th>\n","      <th>genre</th>\n","      <th>opening_theme</th>\n","      <th>ending_theme</th>\n","      <th>duration_min</th>\n","      <th>aired_from_year</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>11013</td>\n","      <td>Inu x Boku SS</td>\n","      <td>Inu X Boku Secret Service</td>\n","      <td>妖狐×僕SS</td>\n","      <td>Youko x Boku SS</td>\n","      <td>https://myanimelist.cdn-dena.com/images/anime/...</td>\n","      <td>TV</td>\n","      <td>Manga</td>\n","      <td>12</td>\n","      <td>Finished Airing</td>\n","      <td>False</td>\n","      <td>Jan 13, 2012 to Mar 30, 2012</td>\n","      <td>{'from': '2012-01-13', 'to': '2012-03-30'}</td>\n","      <td>24 min. per ep.</td>\n","      <td>PG-13 - Teens 13 or older</td>\n","      <td>7.63</td>\n","      <td>139250</td>\n","      <td>1274.0</td>\n","      <td>231</td>\n","      <td>283882</td>\n","      <td>2809</td>\n","      <td>Inu x Boku SS was licensed by Sentai Filmworks...</td>\n","      <td>Winter 2012</td>\n","      <td>Fridays at Unknown</td>\n","      <td>{'Adaptation': [{'mal_id': 17207, 'type': 'man...</td>\n","      <td>Aniplex, Square Enix, Mainichi Broadcasting Sy...</td>\n","      <td>Sentai Filmworks</td>\n","      <td>David Production</td>\n","      <td>Comedy, Supernatural, Romance, Shounen</td>\n","      <td>['\"Nirvana\" by MUCC']</td>\n","      <td>['#1: \"Nirvana\" by MUCC (eps 1, 11-12)', '#2: ...</td>\n","      <td>24.0</td>\n","      <td>2012.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2104</td>\n","      <td>Seto no Hanayome</td>\n","      <td>My Bride is a Mermaid</td>\n","      <td>瀬戸の花嫁</td>\n","      <td>The Inland Sea Bride</td>\n","      <td>https://myanimelist.cdn-dena.com/images/anime/...</td>\n","      <td>TV</td>\n","      <td>Manga</td>\n","      <td>26</td>\n","      <td>Finished Airing</td>\n","      <td>False</td>\n","      <td>Apr 2, 2007 to Oct 1, 2007</td>\n","      <td>{'from': '2007-04-02', 'to': '2007-10-01'}</td>\n","      <td>24 min. per ep.</td>\n","      <td>PG-13 - Teens 13 or older</td>\n","      <td>7.89</td>\n","      <td>91206</td>\n","      <td>727.0</td>\n","      <td>366</td>\n","      <td>204003</td>\n","      <td>2579</td>\n","      <td>NaN</td>\n","      <td>Spring 2007</td>\n","      <td>Unknown</td>\n","      <td>{'Adaptation': [{'mal_id': 759, 'type': 'manga...</td>\n","      <td>TV Tokyo, AIC, Square Enix, Sotsu</td>\n","      <td>Funimation</td>\n","      <td>Gonzo</td>\n","      <td>Comedy, Parody, Romance, School, Shounen</td>\n","      <td>['\"Romantic summer\" by SUN&amp;LUNAR']</td>\n","      <td>['#1: \"Ashita e no Hikari (明日への光)\" by Asuka Hi...</td>\n","      <td>24.0</td>\n","      <td>2007.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5262</td>\n","      <td>Shugo Chara!! Doki</td>\n","      <td>Shugo Chara!! Doki</td>\n","      <td>しゅごキャラ！！どきっ</td>\n","      <td>Shugo Chara Ninenme, Shugo Chara! Second Year</td>\n","      <td>https://myanimelist.cdn-dena.com/images/anime/...</td>\n","      <td>TV</td>\n","      <td>Manga</td>\n","      <td>51</td>\n","      <td>Finished Airing</td>\n","      <td>False</td>\n","      <td>Oct 4, 2008 to Sep 25, 2009</td>\n","      <td>{'from': '2008-10-04', 'to': '2009-09-25'}</td>\n","      <td>24 min. per ep.</td>\n","      <td>PG - Children</td>\n","      <td>7.55</td>\n","      <td>37129</td>\n","      <td>1508.0</td>\n","      <td>1173</td>\n","      <td>70127</td>\n","      <td>802</td>\n","      <td>NaN</td>\n","      <td>Fall 2008</td>\n","      <td>Unknown</td>\n","      <td>{'Adaptation': [{'mal_id': 101, 'type': 'manga...</td>\n","      <td>TV Tokyo, Sotsu</td>\n","      <td>NaN</td>\n","      <td>Satelight</td>\n","      <td>Comedy, Magic, School, Shoujo</td>\n","      <td>['#1: \"Minna no Tamago (みんなのたまご)\" by Shugo Cha...</td>\n","      <td>['#1: \"Rottara Rottara (ロッタラ ロッタラ)\" by Buono! ...</td>\n","      <td>24.0</td>\n","      <td>2008.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>721</td>\n","      <td>Princess Tutu</td>\n","      <td>Princess Tutu</td>\n","      <td>プリンセスチュチュ</td>\n","      <td>NaN</td>\n","      <td>https://myanimelist.cdn-dena.com/images/anime/...</td>\n","      <td>TV</td>\n","      <td>Original</td>\n","      <td>38</td>\n","      <td>Finished Airing</td>\n","      <td>False</td>\n","      <td>Aug 16, 2002 to May 23, 2003</td>\n","      <td>{'from': '2002-08-16', 'to': '2003-05-23'}</td>\n","      <td>16 min. per ep.</td>\n","      <td>PG-13 - Teens 13 or older</td>\n","      <td>8.21</td>\n","      <td>36501</td>\n","      <td>307.0</td>\n","      <td>916</td>\n","      <td>93312</td>\n","      <td>3344</td>\n","      <td>Princess Tutu aired in two parts. The first pa...</td>\n","      <td>Summer 2002</td>\n","      <td>Fridays at Unknown</td>\n","      <td>{'Adaptation': [{'mal_id': 1581, 'type': 'mang...</td>\n","      <td>Memory-Tech, GANSIS, Marvelous AQL</td>\n","      <td>ADV Films</td>\n","      <td>Hal Film Maker</td>\n","      <td>Comedy, Drama, Magic, Romance, Fantasy</td>\n","      <td>['\"Morning Grace\" by Ritsuko Okazaki']</td>\n","      <td>['\"Watashi No Ai Wa Chiisaikeredo\" by Ritsuko ...</td>\n","      <td>16.0</td>\n","      <td>2002.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>12365</td>\n","      <td>Bakuman. 3rd Season</td>\n","      <td>Bakuman.</td>\n","      <td>バクマン。</td>\n","      <td>Bakuman Season 3</td>\n","      <td>https://myanimelist.cdn-dena.com/images/anime/...</td>\n","      <td>TV</td>\n","      <td>Manga</td>\n","      <td>25</td>\n","      <td>Finished Airing</td>\n","      <td>False</td>\n","      <td>Oct 6, 2012 to Mar 30, 2013</td>\n","      <td>{'from': '2012-10-06', 'to': '2013-03-30'}</td>\n","      <td>24 min. per ep.</td>\n","      <td>PG-13 - Teens 13 or older</td>\n","      <td>8.67</td>\n","      <td>107767</td>\n","      <td>50.0</td>\n","      <td>426</td>\n","      <td>182765</td>\n","      <td>2082</td>\n","      <td>NaN</td>\n","      <td>Fall 2012</td>\n","      <td>Unknown</td>\n","      <td>{'Adaptation': [{'mal_id': 9711, 'type': 'mang...</td>\n","      <td>NHK, Shueisha</td>\n","      <td>NaN</td>\n","      <td>J.C.Staff</td>\n","      <td>Comedy, Drama, Romance, Shounen</td>\n","      <td>['#1: \"Moshimo no Hanashi (もしもの話)\" by nano.RIP...</td>\n","      <td>['#1: \"Pride on Everyday\" by Sphere (eps 1-13)...</td>\n","      <td>24.0</td>\n","      <td>2012.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   anime_id                title              title_english title_japanese  \\\n","0     11013        Inu x Boku SS  Inu X Boku Secret Service         妖狐×僕SS   \n","1      2104     Seto no Hanayome      My Bride is a Mermaid          瀬戸の花嫁   \n","2      5262   Shugo Chara!! Doki         Shugo Chara!! Doki    しゅごキャラ！！どきっ   \n","3       721        Princess Tutu              Princess Tutu      プリンセスチュチュ   \n","4     12365  Bakuman. 3rd Season                   Bakuman.          バクマン。   \n","\n","                                  title_synonyms  \\\n","0                                Youko x Boku SS   \n","1                           The Inland Sea Bride   \n","2  Shugo Chara Ninenme, Shugo Chara! Second Year   \n","3                                            NaN   \n","4                               Bakuman Season 3   \n","\n","                                           image_url type    source  episodes  \\\n","0  https://myanimelist.cdn-dena.com/images/anime/...   TV     Manga        12   \n","1  https://myanimelist.cdn-dena.com/images/anime/...   TV     Manga        26   \n","2  https://myanimelist.cdn-dena.com/images/anime/...   TV     Manga        51   \n","3  https://myanimelist.cdn-dena.com/images/anime/...   TV  Original        38   \n","4  https://myanimelist.cdn-dena.com/images/anime/...   TV     Manga        25   \n","\n","            status  airing                  aired_string  \\\n","0  Finished Airing   False  Jan 13, 2012 to Mar 30, 2012   \n","1  Finished Airing   False    Apr 2, 2007 to Oct 1, 2007   \n","2  Finished Airing   False   Oct 4, 2008 to Sep 25, 2009   \n","3  Finished Airing   False  Aug 16, 2002 to May 23, 2003   \n","4  Finished Airing   False   Oct 6, 2012 to Mar 30, 2013   \n","\n","                                        aired         duration  \\\n","0  {'from': '2012-01-13', 'to': '2012-03-30'}  24 min. per ep.   \n","1  {'from': '2007-04-02', 'to': '2007-10-01'}  24 min. per ep.   \n","2  {'from': '2008-10-04', 'to': '2009-09-25'}  24 min. per ep.   \n","3  {'from': '2002-08-16', 'to': '2003-05-23'}  16 min. per ep.   \n","4  {'from': '2012-10-06', 'to': '2013-03-30'}  24 min. per ep.   \n","\n","                      rating  score  scored_by    rank  popularity  members  \\\n","0  PG-13 - Teens 13 or older   7.63     139250  1274.0         231   283882   \n","1  PG-13 - Teens 13 or older   7.89      91206   727.0         366   204003   \n","2              PG - Children   7.55      37129  1508.0        1173    70127   \n","3  PG-13 - Teens 13 or older   8.21      36501   307.0         916    93312   \n","4  PG-13 - Teens 13 or older   8.67     107767    50.0         426   182765   \n","\n","   favorites                                         background    premiered  \\\n","0       2809  Inu x Boku SS was licensed by Sentai Filmworks...  Winter 2012   \n","1       2579                                                NaN  Spring 2007   \n","2        802                                                NaN    Fall 2008   \n","3       3344  Princess Tutu aired in two parts. The first pa...  Summer 2002   \n","4       2082                                                NaN    Fall 2012   \n","\n","            broadcast                                            related  \\\n","0  Fridays at Unknown  {'Adaptation': [{'mal_id': 17207, 'type': 'man...   \n","1             Unknown  {'Adaptation': [{'mal_id': 759, 'type': 'manga...   \n","2             Unknown  {'Adaptation': [{'mal_id': 101, 'type': 'manga...   \n","3  Fridays at Unknown  {'Adaptation': [{'mal_id': 1581, 'type': 'mang...   \n","4             Unknown  {'Adaptation': [{'mal_id': 9711, 'type': 'mang...   \n","\n","                                            producer          licensor  \\\n","0  Aniplex, Square Enix, Mainichi Broadcasting Sy...  Sentai Filmworks   \n","1                  TV Tokyo, AIC, Square Enix, Sotsu        Funimation   \n","2                                    TV Tokyo, Sotsu               NaN   \n","3                 Memory-Tech, GANSIS, Marvelous AQL         ADV Films   \n","4                                      NHK, Shueisha               NaN   \n","\n","             studio                                     genre  \\\n","0  David Production    Comedy, Supernatural, Romance, Shounen   \n","1             Gonzo  Comedy, Parody, Romance, School, Shounen   \n","2         Satelight             Comedy, Magic, School, Shoujo   \n","3    Hal Film Maker    Comedy, Drama, Magic, Romance, Fantasy   \n","4         J.C.Staff           Comedy, Drama, Romance, Shounen   \n","\n","                                       opening_theme  \\\n","0                              ['\"Nirvana\" by MUCC']   \n","1                 ['\"Romantic summer\" by SUN&LUNAR']   \n","2  ['#1: \"Minna no Tamago (みんなのたまご)\" by Shugo Cha...   \n","3             ['\"Morning Grace\" by Ritsuko Okazaki']   \n","4  ['#1: \"Moshimo no Hanashi (もしもの話)\" by nano.RIP...   \n","\n","                                        ending_theme  duration_min  \\\n","0  ['#1: \"Nirvana\" by MUCC (eps 1, 11-12)', '#2: ...          24.0   \n","1  ['#1: \"Ashita e no Hikari (明日への光)\" by Asuka Hi...          24.0   \n","2  ['#1: \"Rottara Rottara (ロッタラ ロッタラ)\" by Buono! ...          24.0   \n","3  ['\"Watashi No Ai Wa Chiisaikeredo\" by Ritsuko ...          16.0   \n","4  ['#1: \"Pride on Everyday\" by Sphere (eps 1-13)...          24.0   \n","\n","   aired_from_year  \n","0           2012.0  \n","1           2007.0  \n","2           2008.0  \n","3           2002.0  \n","4           2012.0  "]},"metadata":{"tags":[]}}]},{"metadata":{"id":"HK7nZqllbccf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":165},"outputId":"c907918f-a9bb-4229-9433-328096fb78cb","executionInfo":{"status":"ok","timestamp":1542487900553,"user_tz":300,"elapsed":348,"user":{"displayName":"DANIEL MARCELO CHAPI ALEJO","photoUrl":"https://lh4.googleusercontent.com/-0qTgCC4gOYw/AAAAAAAAAAI/AAAAAAAAAAo/xbudxPyl1Uk/s64/photo.jpg","userId":"06845776485992914193"}}},"cell_type":"code","source":["animeUserList.head(2)"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>username</th>\n","      <th>anime_id</th>\n","      <th>my_watched_episodes</th>\n","      <th>my_start_date</th>\n","      <th>my_finish_date</th>\n","      <th>my_score</th>\n","      <th>my_status</th>\n","      <th>my_rewatching</th>\n","      <th>my_rewatching_ep</th>\n","      <th>my_last_updated</th>\n","      <th>my_tags</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>karthiga</td>\n","      <td>21</td>\n","      <td>586</td>\n","      <td>0000-00-00</td>\n","      <td>0000-00-00</td>\n","      <td>9</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>2013-03-03 10:52:53</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>karthiga</td>\n","      <td>59</td>\n","      <td>26</td>\n","      <td>0000-00-00</td>\n","      <td>0000-00-00</td>\n","      <td>7</td>\n","      <td>2</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>2013-03-10 13:54:51</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   username  anime_id  my_watched_episodes my_start_date my_finish_date  \\\n","0  karthiga        21                  586    0000-00-00     0000-00-00   \n","1  karthiga        59                   26    0000-00-00     0000-00-00   \n","\n","   my_score  my_status  my_rewatching  my_rewatching_ep      my_last_updated  \\\n","0         9          1            NaN                 0  2013-03-03 10:52:53   \n","1         7          2            NaN                 0  2013-03-10 13:54:51   \n","\n","  my_tags  \n","0     NaN  \n","1     NaN  "]},"metadata":{"tags":[]},"execution_count":14}]},{"metadata":{"id":"7MMbi9HKbem3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":391},"outputId":"fcf2627c-c49a-40d4-cf73-2abdb370f1f1","executionInfo":{"status":"ok","timestamp":1542487901698,"user_tz":300,"elapsed":477,"user":{"displayName":"DANIEL MARCELO CHAPI ALEJO","photoUrl":"https://lh4.googleusercontent.com/-0qTgCC4gOYw/AAAAAAAAAAI/AAAAAAAAAAo/xbudxPyl1Uk/s64/photo.jpg","userId":"06845776485992914193"}}},"cell_type":"code","source":["users.info()"],"execution_count":15,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 199 entries, 0 to 198\n","Data columns (total 17 columns):\n","username                    199 non-null object\n","user_id                     199 non-null int64\n","user_watching               199 non-null int64\n","user_completed              199 non-null int64\n","user_onhold                 199 non-null int64\n","user_dropped                199 non-null int64\n","user_plantowatch            199 non-null int64\n","user_days_spent_watching    199 non-null float64\n","gender                      199 non-null object\n","location                    199 non-null object\n","birth_date                  199 non-null object\n","access_rank                 0 non-null float64\n","join_date                   199 non-null object\n","last_online                 199 non-null object\n","stats_mean_score            199 non-null float64\n","stats_rewatched             199 non-null float64\n","stats_episodes              199 non-null int64\n","dtypes: float64(4), int64(7), object(6)\n","memory usage: 26.5+ KB\n"],"name":"stdout"}]},{"metadata":{"id":"q1MjxTV4bhBE","colab_type":"code","colab":{}},"cell_type":"code","source":["users = users.drop('access_rank', 1)\n","animes['airing'].describe()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Uz6nHX1ocZlC","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","\n","def get_bias_initializer():\n","    return tf.zeros_initializer()\n","\n","def get_weight_initializer():\n","    return tf.random_normal_initializer(mean=0.0, stddev=0.05)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZqA2LuUoccOL","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","num_epoch= 1000\n","\n","batch_size = 16   #Size of the training batch.')\n","\n","learning_rate= 0.0005       #Learning_Rate')\n","\n","l2_reg= False     #L2 regularization.'\n","                            \n","lambda_=0.01      #Wight decay factor.')\n","\n","num_v=3952        #Number of visible neurons (Number of movies the users rated.)')\n","\n","num_h= 128        #Number of hidden neurons\n","\n","num_samples=5953  #Number of training samples (Number of users, who gave a rating)\n","\n","class DAE:\n","    \n","    def __init__(self):\n","        ''' Imlimentation of deep autoencoder class.'''\n","        \n","        self.weight_initializer=get_weight_initializer()\n","        self.bias_initializer=get_bias_initializer()\n","        self.init_parameters()\n","        \n","\n","    def init_parameters(self):\n","        '''Initialize networks weights abd biasis.'''\n","        \n","        with tf.name_scope('weights'):\n","            self.W_1=tf.get_variable(name='weight_1', shape=(num_v,num_h), \n","                                     initializer=self.weight_initializer)\n","            self.W_2=tf.get_variable(name='weight_2', shape=(num_h,num_h), \n","                                     initializer=self.weight_initializer)\n","            self.W_3=tf.get_variable(name='weight_3', shape=(num_h,num_h), \n","                                     initializer=self.weight_initializer)\n","            self.W_4=tf.get_variable(name='weight_4', shape=(num_h,num_v), \n","                                     initializer=self.weight_initializer)\n","        \n","        with tf.name_scope('biases'):\n","            self.b1=tf.get_variable(name='bias_1', shape=(num_h), \n","                                    initializer=self.bias_initializer)\n","            self.b2=tf.get_variable(name='bias_2', shape=(num_h), \n","                                    initializer=self.bias_initializer)\n","            self.b3=tf.get_variable(name='bias_3', shape=(num_h), \n","                                    initializer=self.bias_initializer)\n","        \n","    def _inference(self, x):\n","        ''' Making one forward pass. Predicting the networks outputs.\n","        @param x: input ratings\n","        \n","        @return : networks predictions\n","        '''\n","        \n","        with tf.name_scope('inference'):\n","             a1=tf.nn.sigmoid(tf.nn.bias_add(tf.matmul(x, self.W_1),self.b1))\n","             a2=tf.nn.sigmoid(tf.nn.bias_add(tf.matmul(a1, self.W_2),self.b2))\n","             a3=tf.nn.sigmoid(tf.nn.bias_add(tf.matmul(a2, self.W_3),self.b3))   \n","             a4=tf.matmul(a3, self.W_4) \n","        return a4\n","    \n","    def _compute_loss(self, predictions, labels,num_labels):\n","        ''' Computing the Mean Squared Error loss between the input and output of the network.\n","    \t\t\n","    \t  @param predictions: predictions of the stacked autoencoder\n","    \t  @param labels: input values of the stacked autoencoder which serve as labels at the same time\n","    \t  @param num_labels: number of labels !=0 in the data set to compute the mean\n","    \t\t\n","    \t  @return mean squared error loss tf-operation\n","    \t  '''\n","            \n","        with tf.name_scope('loss'):\n","            \n","            loss_op=tf.div(tf.reduce_sum(tf.square(tf.subtract(predictions,labels))),num_labels)\n","            return loss_op\n","    \t  \n","        \n","\n","    def _optimizer(self, x):\n","        '''Optimization of the network parameter through stochastic gradient descent.\n","            \n","            @param x: input values for the stacked autoencoder.\n","            \n","            @return: tensorflow training operation\n","            @return: ROOT!! mean squared error\n","        '''\n","        \n","        outputs=self._inference(x)\n","        mask=tf.where(tf.equal(x,0.0), tf.zeros_like(x), x) # indices of 0 values in the training set\n","        num_train_labels=tf.cast(tf.count_nonzero(mask),dtype=tf.float32) # number of non zero values in the training set\n","        bool_mask=tf.cast(mask,dtype=tf.bool) # boolean mask\n","        outputs=tf.where(bool_mask, outputs, tf.zeros_like(outputs)) # set the output values to zero if corresponding input values are zero\n","\n","        MSE_loss=self._compute_loss(outputs,x,num_train_labels)\n","        \n","        if l2_reg==True:\n","            l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables()])\n","            MSE_loss = MSE_loss +  lambda_ * l2_loss\n","        \n","        train_op=tf.train.AdamOptimizer(learning_rate).minimize(MSE_loss)\n","        RMSE_loss=tf.sqrt(MSE_loss)\n","\n","        return train_op, RMSE_loss\n","    \n","    def _validation_loss(self, x_train, x_test):\n","        \n","        ''' Computing the loss during the validation time.\n","    \t\t\n","    \t  @param x_train: training data samples\n","    \t  @param x_test: test data samples\n","    \t\t\n","    \t  @return networks predictions\n","    \t  @return root mean squared error loss between the predicted and actual ratings\n","    \t  '''\n","        \n","        outputs=self._inference(x_train) # use training sample to make prediction\n","        mask=tf.where(tf.equal(x_test,0.0), tf.zeros_like(x_test), x_test) # identify the zero values in the test ste\n","        num_test_labels=tf.cast(tf.count_nonzero(mask),dtype=tf.float32) # count the number of non zero values\n","        bool_mask=tf.cast(mask,dtype=tf.bool) \n","        outputs=tf.where(bool_mask, outputs, tf.zeros_like(outputs))\n","    \n","        MSE_loss=self._compute_loss(outputs, x_test, num_test_labels)\n","        RMSE_loss=tf.sqrt(MSE_loss)\n","            \n","        return outputs, RMSE_loss\n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"AZwMi9YrqCWY","colab_type":"code","colab":{}},"cell_type":"code","source":["import os\n","\n","\n","def _get_training_data():  \n","    ''' Buildind the input pipeline for training and inference using TFRecords files.\n","    @return data only for the training\n","    @return data for the inference\n","    '''\n","    \n","    filenames=['/content/drive/My Drive/ML/movieLens/train/'+f for f in os.listdir('/content/drive/My Drive/ML/movieLens/train/')]\n","    \n","    dataset = tf.data.TFRecordDataset(filenames)\n","    dataset = dataset.map(parse)\n","    dataset = dataset.shuffle(buffer_size=500)\n","    dataset = dataset.repeat()\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.prefetch(buffer_size=1)\n","    \n","    dataset2 = tf.data.TFRecordDataset(filenames)\n","    dataset2 = dataset2.map(parse)\n","    dataset2 = dataset2.shuffle(buffer_size=1)\n","    dataset2 = dataset2.repeat()\n","    dataset2 = dataset2.batch(1)\n","    dataset2 = dataset2.prefetch(buffer_size=1)\n","    \n","    return dataset, dataset2\n","    \n","\n","def _get_test_data():\n","    ''' Buildind the input pipeline for test data.'''\n","    \n","    filenames=['/content/drive/My Drive/ML/movieLens/test/'+f for f in os.listdir('/content/drive/My Drive/ML/movieLens/test/')]\n","    \n","    dataset = tf.data.TFRecordDataset(filenames)\n","    dataset = dataset.map(parse)\n","    dataset = dataset.shuffle(buffer_size=1)\n","    dataset = dataset.repeat()\n","    dataset = dataset.batch(1)\n","    dataset = dataset.prefetch(buffer_size=1)\n","    \n","    return dataset\n","\n","\n","def parse(serialized):\n","    ''' Parser fot the TFRecords file.'''\n","    \n","    features={'movie_ratings':tf.FixedLenFeature([3952], tf.float32),  \n","              }\n","    parsed_example=tf.parse_single_example(serialized,\n","                                           features=features,\n","                                           )\n","    movie_ratings = tf.cast(parsed_example['movie_ratings'], tf.float32)\n","     \n","    return movie_ratings"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kxjPoz8Uc0jb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":724},"outputId":"67162007-6761-4a50-e921-1ac20733e529","executionInfo":{"status":"error","timestamp":1542493545053,"user_tz":300,"elapsed":4098,"user":{"displayName":"DANIEL MARCELO CHAPI ALEJO","photoUrl":"https://lh4.googleusercontent.com/-0qTgCC4gOYw/AAAAAAAAAAI/AAAAAAAAAAo/xbudxPyl1Uk/s64/photo.jpg","userId":"06845776485992914193"}}},"cell_type":"code","source":["\n","import numpy as np\n","\n","\n","\n","\n","#tf.app.run()\n","\n","'''Building the graph, opening of a session and starting the training od the neural network.'''\n","\n","num_batches=int(num_samples/batch_size)\n","\n","with tf.Graph().as_default():\n","\n","    train_data, train_data_infer=_get_training_data()\n","    test_data=_get_test_data()\n","    print(train_data)\n","    \n","    iter_train = train_data.make_initializable_iterator()\n","    iter_train_infer=train_data_infer.make_initializable_iterator()\n","    iter_test=test_data.make_initializable_iterator()\n","\n","    x_train= iter_train.get_next()\n","    x_train_infer=iter_train_infer.get_next()\n","    x_test=iter_test.get_next()\n","\n","    model=DAE()\n","\n","    train_op, train_loss_op=model._optimizer(x_train)\n","    pred_op, test_loss_op=model._validation_loss(x_train_infer, x_test)\n","\n","    with tf.Session() as sess:\n","\n","        sess.run(tf.global_variables_initializer())\n","        train_loss=0\n","        test_loss=0\n","\n","        for epoch in range(num_epoch):\n","\n","            sess.run(iter_train.initializer)\n","\n","            for batch_nr in range(num_batches):\n","\n","                _, loss_=sess.run((train_op, train_loss_op))\n","                train_loss+=loss_\n","\n","            sess.run(iter_train_infer.initializer)\n","            sess.run(iter_test.initializer)\n","\n","            for i in range(num_samples):\n","                pred, loss_=sess.run((pred_op, test_loss_op))\n","                test_loss+=loss_\n","\n","            print('epoch_nr: %i, train_loss: %.3f, test_loss: %.3f'%(epoch,(train_loss/num_batches),(test_loss/num_samples)))\n","            train_loss=0\n","            test_loss=0\n","\n","    \n","    \n"],"execution_count":48,"outputs":[{"output_type":"stream","text":["<PrefetchDataset shapes: (?, 3952), types: tf.float32>\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-48-4ca1134f0fb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                 \u001b[0mtest_loss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mloss_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1155\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetch_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmake_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mbuild_results\u001b[0;34m(self, session, tensor_values)\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0;31m# If the fetch was in the feeds, use the fed value, otherwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;31m# use the returned value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_handles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m           \u001b[0;31m# A fetch had a corresponding direct TensorHandle feed. Call eval()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           \u001b[0;31m# to obtain the Tensor value from the TensorHandle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    619\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m     \u001b[0;31m# Necessary to support Python's collection membership operators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"mLaF_uDDp6S8","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}